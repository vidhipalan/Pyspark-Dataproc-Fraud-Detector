import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when
from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline

# â”€â”€â”€ 0) If running in a notebook, stop any active session â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from pyspark.sql import SparkSession as _SS
    if _SS.getActiveSession():
        _SS.getActiveSession().stop()
except:
    pass

# â”€â”€â”€ 1) Start Spark in local mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
spark = (
    SparkSession
    .builder
    .master("local[*]")
    .appName("RetrainFraudRFFullPipeline")
    .getOrCreate()
)

# â”€â”€â”€ 2) Load raw data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
raw = spark.read.csv(
    "gs://fraud_detection_dataset_1/Synthetic_Financial_datasets_log.csv",
    header=True,
    inferSchema=True
)

# â”€â”€â”€ 3) Initial feature engineering â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
df = (raw
      .drop("nameOrig", "nameDest", "isFlaggedFraud", "step")
      .withColumn("deltaOrig",           col("oldbalanceOrg")  - col("newbalanceOrig"))
      .withColumn("deltaDest",           col("newbalanceDest") - col("oldbalanceDest"))
      .withColumn("amount_to_orig_ratio", col("amount")/(col("oldbalanceOrg")+1))
      .withColumn("amount_to_dest_ratio", col("amount")/(col("oldbalanceDest")+1))
     )

# â”€â”€â”€ 4) Sample & weight â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
full  = df.stat.sampleBy("isFraud", {0: 1.0, 1: 1.0}, seed=2025)
train = full.stat.sampleBy("isFraud", {0: 0.8, 1: 0.8}, seed=2025)

total  = train.count()
frauds = train.filter(col("isFraud") == 1).count()
w      = 1.0 / (frauds / total)

train = train.withColumn(
    "weight",
    when(col("isFraud") == 1, w).otherwise(1.0)
)

# â”€â”€â”€ 5) Define full pipeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
string_indexer = StringIndexer(
    inputCol="type", outputCol="type_idx", handleInvalid="keep"
)
onehot_encoder = OneHotEncoder(
    inputCols=["type_idx"], outputCols=["type_vec"]
)
assembler      = VectorAssembler(
    inputCols=[
        'amount','oldbalanceOrg','newbalanceOrig',
        'oldbalanceDest','newbalanceDest',
        'deltaOrig','deltaDest',
        'amount_to_orig_ratio','amount_to_dest_ratio',
        'type_vec'
    ],
    outputCol="rawFeatures"
)
scaler         = StandardScaler(
    inputCol="rawFeatures", outputCol="features",
    withStd=True, withMean=False
)
rf             = RandomForestClassifier(
    labelCol="isFraud",
    featuresCol="features",
    weightCol="weight",
    numTrees=100,
    maxDepth=8,
    seed=42
)

pipeline = Pipeline(stages=[
    string_indexer,
    onehot_encoder,
    assembler,
    scaler,
    rf
])

# â”€â”€â”€ 6) Fit & time â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
start = time.time()
model = pipeline.fit(train)
print(f"ðŸ”¨ TRAIN TIME: {time.time() - start:.2f} s")

# â”€â”€â”€ 7) Save to GCS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
model_path = "gs://fraud_detection_dataset_1/rf_100pct_model"
model.write().overwrite().save(model_path)
print(f"âœ… Full pipeline saved to: {model_path}")

# â”€â”€â”€ 8) Done â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
spark.stop()

