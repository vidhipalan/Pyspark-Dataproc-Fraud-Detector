{"cells": [{"cell_type": "code", "execution_count": 1, "id": "57065c38-a141-4e81-b7b5-d3a244cc7e15", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql.functions import col, when\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\nimport time"}, {"cell_type": "code", "execution_count": 2, "id": "1cb8a976-0256-4b5d-a257-4675119be85a", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "raw = spark.read.csv(\n    \"gs://fraud_detection_dataset_1/Synthetic_Financial_datasets_log.csv\",\n    header=True, inferSchema=True\n)"}, {"cell_type": "code", "execution_count": 3, "id": "1c019ae6-da03-4390-b293-770d78af2a95", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "df = (raw\n      .drop(\"nameOrig\", \"nameDest\", \"isFlaggedFraud\", \"step\")\n      .withColumn(\"deltaOrig\",           col(\"oldbalanceOrg\")  - col(\"newbalanceOrig\"))\n      .withColumn(\"deltaDest\",           col(\"newbalanceDest\") - col(\"oldbalanceDest\"))\n      .withColumn(\"amount_to_orig_ratio\", col(\"amount\")/(col(\"oldbalanceOrg\")+1))\n      .withColumn(\"amount_to_dest_ratio\", col(\"amount\")/(col(\"oldbalanceDest\")+1))\n     )\nsi_model    = StringIndexer(inputCol=\"type\", outputCol=\"type_idx\", handleInvalid=\"keep\").fit(df)\ndf_indexed  = si_model.transform(df)\nohe_model   = OneHotEncoder(inputCols=[\"type_idx\"], outputCols=[\"type_vec\"]).fit(df_indexed)\ndf_encoded  = ohe_model.transform(df_indexed)"}, {"cell_type": "code", "execution_count": 4, "id": "1cd366ac-ca62-4188-9095-e94b33a8b8d9", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "full   = df_encoded.stat.sampleBy(\"isFraud\", {0:1.0, 1:1.0}, seed=2025)\ntrain  = full.stat.sampleBy(\"isFraud\", {0:0.8, 1:0.8}, seed=2025)\ntotal  = train.count()\nfrauds = train.filter(col(\"isFraud\")==1).count()\nw      = 1.0/(frauds/total)\ntrain  = train.withColumn(\"weight\", when(col(\"isFraud\")==1, w).otherwise(1.0))\nfeature_cols = [\n    'amount','oldbalanceOrg','newbalanceOrig',\n    'oldbalanceDest','newbalanceDest',\n    'deltaOrig','deltaDest',\n    'amount_to_orig_ratio','amount_to_dest_ratio',\n    'type_vec'\n]\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"rawFeatures\")\nscaler    = StandardScaler(inputCol=\"rawFeatures\", outputCol=\"features\", withStd=True, withMean=False)\nrf        = RandomForestClassifier(labelCol=\"isFraud\", featuresCol=\"features\",\n                                   weightCol=\"weight\", numTrees=100, maxDepth=8, seed=42)\n\npipeline = Pipeline(stages=[assembler, scaler, rf])"}, {"cell_type": "code", "execution_count": 5, "id": "a0d3d75c-15e7-4db3-b1b6-a662283fd62f", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/04 00:36:32 WARN DAGScheduler: Broadcasting large task binary with size 1294.2 KiB\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "100% sample \u2014 TRAIN TIME: 382.13 sec\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Model saved to: gs://fraud_detection_dataset_1/rf_100pct_model\n"}], "source": "start = time.time()\nmodel = pipeline.fit(train)\nprint(f\"100% sample \u2014 TRAIN TIME: {time.time() - start:.2f} sec\")\nmodel_path = \"gs://fraud_detection_dataset_1/rf_100pct_model\"\nmodel.write().overwrite().save(model_path)\nprint(f\"Model saved to: {model_path}\")"}, {"cell_type": "code", "execution_count": 6, "id": "e68ebcf8-9c49-4299-8e4b-bb4f8779d4e9", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/lib/spark/python/pyspark/sql/context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n  warnings.warn(\n25/05/04 00:40:08 WARN DAGScheduler: Broadcasting large task binary with size 1106.5 KiB\n25/05/04 00:40:13 WARN DAGScheduler: Broadcasting large task binary with size 1119.1 KiB\n25/05/04 00:40:28 WARN DAGScheduler: Broadcasting large task binary with size 1111.1 KiB\n"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy: 0.9999\n=== Confusion Matrix ===\n[[1.235633e+06 1.250000e+02]\n [7.000000e+00 1.658000e+03]]\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 99:================================================>         (5 + 1) / 6]\r"}, {"name": "stdout", "output_type": "stream", "text": "\n=== Classification Report ===\nClass 0 \u2014 precision: 1.0000, recall: 0.9999, f1-score: 0.9999, support: 1237080\nClass 1 \u2014 precision: 0.9299, recall: 0.9958, f1-score: 0.9617, support: 1672\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import col\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.functions import vector_to_array\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nmodel = PipelineModel.load(\"gs://fraud_detection_dataset_1/rf_100pct_model\")\nraw = spark.read.csv(\"gs://fraud_detection_dataset_1/Synthetic_Financial_datasets_log.csv\",header=True, inferSchema=True)\ndf = (raw\n      .drop(\"nameOrig\", \"nameDest\", \"isFlaggedFraud\", \"step\")\n      .withColumn(\"deltaOrig\",           col(\"oldbalanceOrg\")  - col(\"newbalanceOrig\"))\n      .withColumn(\"deltaDest\",           col(\"newbalanceDest\") - col(\"oldbalanceDest\"))\n      .withColumn(\"amount_to_orig_ratio\", col(\"amount\")/(col(\"oldbalanceOrg\")+1))\n      .withColumn(\"amount_to_dest_ratio\", col(\"amount\")/(col(\"oldbalanceDest\")+1))\n)\nsi_model   = StringIndexer(inputCol=\"type\", outputCol=\"type_idx\", handleInvalid=\"keep\").fit(df)\ndf_indexed = si_model.transform(df)\nohe_model  = OneHotEncoder(inputCols=[\"type_idx\"], outputCols=[\"type_vec\"]).fit(df_indexed)\ndf_encoded = ohe_model.transform(df_indexed)\n\nfull  = df_encoded.stat.sampleBy(\"isFraud\", {0:1.0, 1:1.0}, seed=2025)\ntrain = full.stat.sampleBy(\"isFraud\", {0:0.8, 1:0.8}, seed=2025)\ntest  = full.subtract(train)\npreds = (model.transform(test)\n               .withColumn(\"prob_arr\", vector_to_array(col(\"probability\")))\n               .withColumn(\"pred_label\",\n                           when(col(\"prob_arr\")[1] > 0.5, 1).otherwise(0))\n)\npred_and_lbl = preds.select(\"pred_label\",\"isFraud\") \\\n                    .rdd.map(lambda r: (float(r[0]), float(r[1])))\nmetrics = MulticlassMetrics(pred_and_lbl)\n\nprint(f\"Accuracy: {metrics.accuracy:.4f}\")\nprint(\"=== Confusion Matrix ===\")\nprint(metrics.confusionMatrix().toArray())\nlabels = sorted(pred_and_lbl.map(lambda x: x[1]).distinct().collect())\nsupport = dict(test.groupBy(\"isFraud\").count().rdd.map(lambda r: (float(r[0]), r[1])).collect())\n\nprint(\"\\n=== Classification Report ===\")\nfor lbl in labels:\n    print(f\"Class {int(lbl)} \u2014 precision: {metrics.precision(lbl):.4f}, \"\n          f\"recall: {metrics.recall(lbl):.4f}, \"\n          f\"f1-score: {metrics.fMeasure(lbl):.4f}, \"\n          f\"support: {support.get(lbl,0)}\")"}, {"cell_type": "code", "execution_count": 7, "id": "3e18b2b7-a8e3-4fbd-b559-32b518c77025", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/04 00:53:17 WARN DAGScheduler: Broadcasting large task binary with size 1273.2 KiB\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "75% sample \u2014 TRAIN TIME: 240.86 sec\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Model saved to: gs://fraud_detection_dataset_1/rf_75pct_model\n"}], "source": "from pyspark.sql.functions import col, when\nfrom pyspark.ml.feature import (\n    StringIndexer, OneHotEncoder,\n    VectorAssembler, StandardScaler\n)\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml import PipelineModel\nfrom pyspark.ml.functions import vector_to_array\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nimport time\n\n# 1) Load & feature\u2011engineer\nraw = spark.read.csv(\n    \"gs://fraud_detection_dataset_1/Synthetic_Financial_datasets_log.csv\",\n    header=True, inferSchema=True\n)\ndf = (raw\n      .drop(\"nameOrig\",\"nameDest\",\"isFlaggedFraud\",\"step\")\n      .withColumn(\"deltaOrig\", col(\"oldbalanceOrg\")  - col(\"newbalanceOrig\"))\n      .withColumn(\"deltaDest\", col(\"newbalanceDest\") - col(\"oldbalanceDest\"))\n      .withColumn(\"amount_to_orig_ratio\", col(\"amount\")/(col(\"oldbalanceOrg\")+1))\n      .withColumn(\"amount_to_dest_ratio\",   col(\"amount\")/(col(\"oldbalanceDest\")+1))\n)\n\n# 2) Encode & assemble\nsi_model    = StringIndexer(inputCol=\"type\", outputCol=\"type_idx\", handleInvalid=\"keep\").fit(df)\ndf_indexed  = si_model.transform(df)\nohe_model   = OneHotEncoder(inputCols=[\"type_idx\"], outputCols=[\"type_vec\"]).fit(df_indexed)\ndf_encoded  = ohe_model.transform(df_indexed)\n\n# 3) **Stratified 75% sample**, then 80/20 split\nsample75 = df_encoded.stat.sampleBy(\"isFraud\", {0:0.75, 1:0.75}, seed=2025)\ntrain75  = sample75.stat.sampleBy(\"isFraud\", {0:0.8, 1:0.8}, seed=2025)\ntest75   = sample75.subtract(train75)\n\n# 4) Compute class weights on TRAIN75\ntotal75  = train75.count()\nfrauds75 = train75.filter(col(\"isFraud\")==1).count()\nw75      = 1.0/(frauds75/total75)\ntrain75  = train75.withColumn(\"weight\", when(col(\"isFraud\")==1, w75).otherwise(1.0))\n\n# 5) Build pipeline\nfeature_cols = [\n    'amount','oldbalanceOrg','newbalanceOrig',\n    'oldbalanceDest','newbalanceDest',\n    'deltaOrig','deltaDest',\n    'amount_to_orig_ratio','amount_to_dest_ratio',\n    'type_vec'\n]\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"rawFeatures\")\nscaler    = StandardScaler(inputCol=\"rawFeatures\", outputCol=\"features\",\n                            withStd=True, withMean=False)\nrf75      = RandomForestClassifier(\n    labelCol=\"isFraud\", featuresCol=\"features\", weightCol=\"weight\",\n    numTrees=100, maxDepth=8, seed=42\n)\npipeline75 = Pipeline(stages=[assembler, scaler, rf75])\n\n# 6) Train & time on 75% sample\nstart = time.time()\nmodel75 = pipeline75.fit(train75)\nprint(f\"75% sample \u2014 TRAIN TIME: {time.time() - start:.2f} sec\")\n\n# 7) Save the 75% model\nmodel_path75 = \"gs://fraud_detection_dataset_1/rf_75pct_model\"\nmodel75.write().overwrite().save(model_path75)\nprint(f\"Model saved to: {model_path75}\")"}, {"cell_type": "code", "execution_count": 9, "id": "5ac0cb8e-57aa-4cf0-9a7d-957ab3c7f45e", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/04 00:55:54 WARN DAGScheduler: Broadcasting large task binary with size 1027.5 KiB\n25/05/04 00:55:54 WARN DAGScheduler: Broadcasting large task binary with size 1039.9 KiB\n25/05/04 00:56:01 WARN DAGScheduler: Broadcasting large task binary with size 1032.1 KiB\n"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy: 0.9999\n=== Confusion Matrix ===\n[[9.33532e+05 9.00000e+01]\n [2.00000e+00 1.25600e+03]]\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 211:=================================================>       (7 + 1) / 8]\r"}, {"name": "stdout", "output_type": "stream", "text": "\n=== Classification Report ===\nClass 0 \u2014 precision: 1.0000, recall:    0.9999, f1-score:  1.0000, support:   933622\nClass 1 \u2014 precision: 0.9331, recall:    0.9984, f1-score:  0.9647, support:   1258\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "preds75 = (\n    model75.transform(test75)\n           .withColumn(\"prob_arr\", vector_to_array(col(\"probability\")))\n           .withColumn(\"pred_label\",\n               when(col(\"prob_arr\")[1] > 0.5, 1).otherwise(0))\n)\n\nrdd75 = preds75.select(\"pred_label\",\"isFraud\") \\\n               .rdd.map(lambda r: (float(r[0]), float(r[1])))\nmetrics75 = MulticlassMetrics(rdd75)\n\nprint(f\"Accuracy: {metrics75.accuracy:.4f}\")\nprint(\"=== Confusion Matrix ===\")\nprint(metrics75.confusionMatrix().toArray())\n\n# Classification report\nlabels75  = sorted(rdd75.map(lambda x: x[1]).distinct().collect())\nsupport75 = dict(test75.groupBy(\"isFraud\").count().rdd.map(lambda r: (float(r[0]), r[1])).collect())\nprint(\"\\n=== Classification Report ===\")\nfor lbl in labels75:\n    print(f\"Class {int(lbl)} \u2014 \"\n          f\"precision: {metrics75.precision(lbl):.4f}, \"\n          f\"recall:    {metrics75.recall(lbl):.4f}, \"\n          f\"f1-score:  {metrics75.fMeasure(lbl):.4f}, \"\n          f\"support:   {support75.get(lbl,0)}\")"}, {"cell_type": "code", "execution_count": 10, "id": "f6e6e6c4-d9b9-406b-8465-50faee075e40", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/05/04 01:02:18 WARN DAGScheduler: Broadcasting large task binary with size 1132.3 KiB\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "50% sample \u2014 TRAIN TIME: 167.04 sec\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Model saved to: gs://fraud_detection_dataset_1/rf_50pct_model\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy: 1.0000\n=== Confusion Matrix ===\n[[6.25801e+05 1.50000e+01]\n [2.00000e+00 8.51000e+02]]\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 298:===================>                                     (2 + 4) / 6]\r"}, {"name": "stdout", "output_type": "stream", "text": "\n=== Classification Report ===\nClass 0 \u2014 precision: 1.0000, recall:    1.0000, f1-score:  1.0000, support:   626015\nClass 1 \u2014 precision: 0.9827, recall:    0.9977, f1-score:  0.9901, support:   806\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import col, when\nfrom pyspark.ml.feature import (\n    StringIndexer, OneHotEncoder,\n    VectorAssembler, StandardScaler\n)\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.functions import vector_to_array\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nimport time\n\n# 1) Load & feature-engineer\nraw = spark.read.csv(\n    \"gs://fraud_detection_dataset_1/Synthetic_Financial_datasets_log.csv\",\n    header=True, inferSchema=True\n)\ndf = (raw\n      .drop(\"nameOrig\",\"nameDest\",\"isFlaggedFraud\",\"step\")\n      .withColumn(\"deltaOrig\", col(\"oldbalanceOrg\")  - col(\"newbalanceOrig\"))\n      .withColumn(\"deltaDest\", col(\"newbalanceDest\") - col(\"oldbalanceDest\"))\n      .withColumn(\"amount_to_orig_ratio\", col(\"amount\")/(col(\"oldbalanceOrg\")+1))\n      .withColumn(\"amount_to_dest_ratio\",   col(\"amount\")/(col(\"oldbalanceDest\")+1))\n)\n\n# 2) Encode & assemble\nsi_model    = StringIndexer(inputCol=\"type\", outputCol=\"type_idx\", handleInvalid=\"keep\").fit(df)\ndf_indexed  = si_model.transform(df)\nohe_model   = OneHotEncoder(inputCols=[\"type_idx\"], outputCols=[\"type_vec\"]).fit(df_indexed)\ndf_encoded  = ohe_model.transform(df_indexed)\n\n# 3) **Stratified 50% sample**, then 80/20 split\nsample50 = df_encoded.stat.sampleBy(\"isFraud\", {0: 0.50, 1: 0.50}, seed=2025)\ntrain50  = sample50.stat.sampleBy(\"isFraud\", {0: 0.80, 1: 0.80}, seed=2025)\ntest50   = sample50.subtract(train50)\n\n# 4) Compute class weights on TRAIN50\ntotal50  = train50.count()\nfrauds50 = train50.filter(col(\"isFraud\") == 1).count()\nw50      = 1.0 / (frauds50 / total50)\ntrain50  = train50.withColumn(\"weight\", when(col(\"isFraud\") == 1, w50).otherwise(1.0))\n\n# 5) Build pipeline\nfeature_cols = [\n    'amount','oldbalanceOrg','newbalanceOrig',\n    'oldbalanceDest','newbalanceDest',\n    'deltaOrig','deltaDest',\n    'amount_to_orig_ratio','amount_to_dest_ratio',\n    'type_vec'\n]\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"rawFeatures\")\nscaler    = StandardScaler(inputCol=\"rawFeatures\", outputCol=\"features\",\n                            withStd=True, withMean=False)\nrf50      = RandomForestClassifier(\n    labelCol=\"isFraud\", featuresCol=\"features\", weightCol=\"weight\",\n    numTrees=100, maxDepth=8, seed=42\n)\npipeline50 = Pipeline(stages=[assembler, scaler, rf50])\n\n# 6) Train & time on 50% sample\nstart = time.time()\nmodel50 = pipeline50.fit(train50)\nprint(f\"50% sample \u2014 TRAIN TIME: {time.time() - start:.2f} sec\")\n\n# 7) Save the 50% model\nmodel_path50 = \"gs://fraud_detection_dataset_1/rf_50pct_model\"\nmodel50.write().overwrite().save(model_path50)\nprint(f\"Model saved to: {model_path50}\")\n\n# 8) Predict & metrics\npreds50 = (\n    model50.transform(test50)\n           .withColumn(\"prob_arr\", vector_to_array(col(\"probability\")))\n           .withColumn(\"pred_label\",\n               when(col(\"prob_arr\")[1] > 0.5, 1).otherwise(0))\n)\n\nrdd50 = preds50.select(\"pred_label\",\"isFraud\") \\\n               .rdd.map(lambda r: (float(r[0]), float(r[1])))\nmetrics50 = MulticlassMetrics(rdd50)\n\nprint(f\"Accuracy: {metrics50.accuracy:.4f}\")\nprint(\"=== Confusion Matrix ===\")\nprint(metrics50.confusionMatrix().toArray())\n\n# Classification report\nlabels50  = sorted(rdd50.map(lambda x: x[1]).distinct().collect())\nsupport50 = dict(test50.groupBy(\"isFraud\").count().rdd.map(lambda r: (float(r[0]), r[1])).collect())\nprint(\"\\n=== Classification Report ===\")\nfor lbl in labels50:\n    print(f\"Class {int(lbl)} \u2014 \"\n          f\"precision: {metrics50.precision(lbl):.4f}, \"\n          f\"recall:    {metrics50.recall(lbl):.4f}, \"\n          f\"f1-score:  {metrics50.fMeasure(lbl):.4f}, \"\n          f\"support:   {support50.get(lbl,0)}\")\n"}, {"cell_type": "code", "execution_count": 11, "id": "5b9ecea2-3a41-4066-846c-ff9e92277210", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "20% sample \u2014 TRAIN TIME: 68.70 sec\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Model saved to: gs://fraud_detection_dataset_1/rf_20pct_model\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "Accuracy: 1.0000\n=== Confusion Matrix ===\n[[2.52905e+05 9.00000e+00]\n [0.00000e+00 3.25000e+02]]\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 377:===============================================>         (5 + 1) / 6]\r"}, {"name": "stdout", "output_type": "stream", "text": "\n=== Classification Report ===\nClass 0 \u2014 precision: 1.0000, recall:    1.0000, f1-score:  1.0000, support:   252914\nClass 1 \u2014 precision: 0.9731, recall:    1.0000, f1-score:  0.9863, support:   325\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.functions import col, when\nfrom pyspark.ml.feature import (\n    StringIndexer, OneHotEncoder,\n    VectorAssembler, StandardScaler\n)\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.functions import vector_to_array\nfrom pyspark.mllib.evaluation import MulticlassMetrics\nimport time\n\n# 1) Load & feature-engineer\nraw = spark.read.csv(\n    \"gs://fraud_detection_dataset_1/Synthetic_Financial_datasets_log.csv\",\n    header=True, inferSchema=True\n)\ndf = (raw\n      .drop(\"nameOrig\",\"nameDest\",\"isFlaggedFraud\",\"step\")\n      .withColumn(\"deltaOrig\", col(\"oldbalanceOrg\")  - col(\"newbalanceOrig\"))\n      .withColumn(\"deltaDest\", col(\"newbalanceDest\") - col(\"oldbalanceDest\"))\n      .withColumn(\"amount_to_orig_ratio\", col(\"amount\")/(col(\"oldbalanceOrg\")+1))\n      .withColumn(\"amount_to_dest_ratio\",   col(\"amount\")/(col(\"oldbalanceDest\")+1))\n)\n\n# 2) Encode & assemble\nsi_model    = StringIndexer(inputCol=\"type\", outputCol=\"type_idx\", handleInvalid=\"keep\").fit(df)\ndf_indexed  = si_model.transform(df)\nohe_model   = OneHotEncoder(inputCols=[\"type_idx\"], outputCols=[\"type_vec\"]).fit(df_indexed)\ndf_encoded  = ohe_model.transform(df_indexed)\n\n# 3) **Stratified 20% sample**, then 80/20 split\nsample20 = df_encoded.stat.sampleBy(\"isFraud\", {0: 0.20, 1: 0.20}, seed=2025)\ntrain20  = sample20.stat.sampleBy(\"isFraud\", {0: 0.80, 1: 0.80}, seed=2025)\ntest20   = sample20.subtract(train20)\n\n# 4) Compute class weights on TRAIN20\ntotal20  = train20.count()\nfrauds20 = train20.filter(col(\"isFraud\") == 1).count()\nw20      = 1.0 / (frauds20 / total20)\ntrain20  = train20.withColumn(\"weight\", when(col(\"isFraud\") == 1, w20).otherwise(1.0))\n\n# 5) Build pipeline\nfeature_cols = [\n    'amount','oldbalanceOrg','newbalanceOrig',\n    'oldbalanceDest','newbalanceDest',\n    'deltaOrig','deltaDest',\n    'amount_to_orig_ratio','amount_to_dest_ratio',\n    'type_vec'\n]\nassembler = VectorAssembler(inputCols=feature_cols, outputCol=\"rawFeatures\")\nscaler    = StandardScaler(inputCol=\"rawFeatures\", outputCol=\"features\",\n                            withStd=True, withMean=False)\nrf20      = RandomForestClassifier(\n    labelCol=\"isFraud\", featuresCol=\"features\", weightCol=\"weight\",\n    numTrees=100, maxDepth=8, seed=42\n)\npipeline20 = Pipeline(stages=[assembler, scaler, rf20])\n\n# 6) Train & time on 20% sample\nstart = time.time()\nmodel20 = pipeline20.fit(train20)\nprint(f\"20% sample \u2014 TRAIN TIME: {time.time() - start:.2f} sec\")\n\n# 7) Save the 20% model\nmodel_path20 = \"gs://fraud_detection_dataset_1/rf_20pct_model\"\nmodel20.write().overwrite().save(model_path20)\nprint(f\"Model saved to: {model_path20}\")\n\n# 8) Predict & metrics\npreds20 = (\n    model20.transform(test20)\n            .withColumn(\"prob_arr\", vector_to_array(col(\"probability\")))\n            .withColumn(\"pred_label\",\n                when(col(\"prob_arr\")[1] > 0.5, 1).otherwise(0))\n)\n\nrdd20 = preds20.select(\"pred_label\",\"isFraud\") \\\n               .rdd.map(lambda r: (float(r[0]), float(r[1])))\nmetrics20 = MulticlassMetrics(rdd20)\n\nprint(f\"Accuracy: {metrics20.accuracy:.4f}\")\nprint(\"=== Confusion Matrix ===\")\nprint(metrics20.confusionMatrix().toArray())\n\n# Classification report\nlabels20  = sorted(rdd20.map(lambda x: x[1]).distinct().collect())\nsupport20 = dict(test20.groupBy(\"isFraud\").count().rdd.map(lambda r: (float(r[0]), r[1])).collect())\nprint(\"\\n=== Classification Report ===\")\nfor lbl in labels20:\n    print(f\"Class {int(lbl)} \u2014 \"\n          f\"precision: {metrics20.precision(lbl):.4f}, \"\n          f\"recall:    {metrics20.recall(lbl):.4f}, \"\n          f\"f1-score:  {metrics20.fMeasure(lbl):.4f}, \"\n          f\"support:   {support20.get(lbl,0)}\")\n"}, {"cell_type": "code", "execution_count": null, "id": "92ec9944-2db2-42a1-911d-c7b2a5d2c045", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}